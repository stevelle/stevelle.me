<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/"><channel><title>Bad at all of this (Posts about thinkings)</title><link>https://stevelle.me/</link><description></description><atom:link href="https://stevelle.me/categories/thinkings.xml" type="application/rss+xml" rel="self"></atom:link><language>en</language><lastBuildDate>Sun, 23 Apr 2017 21:04:40 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>Learning As Performance, part 2</title><link>https://stevelle.me/posts/learning-as-performance-2/</link><dc:creator>Steve Lewis</dc:creator><description>&lt;div&gt;&lt;p&gt;In the first part &lt;a class="footnote-reference" href="https://stevelle.me/posts/learning-as-performance-2/#id2" id="id1"&gt;[1]&lt;/a&gt;, I explored the topic of performance reviews, and I
proposed that learnings could be a meaningful and constructive review criteria.
This time I want to explore ways to recognize and keep track of the individual
learnings so they are available at performance review time.&lt;/p&gt;
&lt;div class="section" id="record-keeping"&gt;
&lt;h2&gt;Record Keeping&lt;/h2&gt;
&lt;p&gt;To give a performance review based on "things learned" it is important to have
the list of "things". This gets to one of the hardest parts about
self-assessment for me: keeping a good record of activity. That activity can
take many forms, not limited to:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul class="simple"&gt;
&lt;li&gt;close-up actions like applying a point change for a performance issue
triggered by learning more about some technical topic&lt;/li&gt;
&lt;li&gt;inspired decisions like influencing the modeling of a component based on
learning about the nature of the vendors your customers work with&lt;/li&gt;
&lt;li&gt;big picture perspectives like from recognizing patterns in bug reports which
helps to justify important rework to improve product quality and lower
support costs&lt;/li&gt;
&lt;li&gt;thematic concerns such as developing excellent insights for code reviews,
spurred by accumulated experiences about how the product changes over time,
when leads to coding decisions which make long term maintenance and change
easier&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;Some of these are easier to recognize in the moment and possibly to quantify,
but at the same time harder to keep track of or value over several months to a
year later when it comes time for a performance review.&lt;/p&gt;
&lt;p&gt;Others take a longer view to recognize occuring, and sometimes even longer views
to fully realize the value from (and quantifying the value can get really sticky
as well). In those cases it can be helpful to have a good list of things you
have been doing and to have someone to review them with as a collaborative
effort to recognize and fully appreciate.&lt;/p&gt;
&lt;dl class="docutils"&gt;
&lt;dt&gt;It is important to articulate two things for each thing brought to a review:&lt;/dt&gt;
&lt;dd&gt;&lt;ul class="first last simple"&gt;
&lt;li&gt;The magnitude of the value gained, and the learning that facilitated it. An
estimate is more valuable than abstract language but hard metrics are best.&lt;/li&gt;
&lt;li&gt;Where the thinking, the learning, and the lessons were shared. Shared
learning applies a value multiplier.&lt;/li&gt;
&lt;/ul&gt;
&lt;/dd&gt;
&lt;/dl&gt;
&lt;p&gt;A scenario like the first stands to benefit the whole team if the technical
learning is shared and the team gains the ability to recognize the cause and to
apply a solution to similar performance issues. A brown-bag seminar could be a
great way to share that learning, and the calendar event serves as a record of
the activity.&lt;/p&gt;
&lt;p&gt;In the second scenario reflects a decision made in the normal course of doing
the work. It is possible that commit message or notes in a bug tracker might
capture some of the insight and reasoning behind it. The insight and the
resulting model likely needs to be reviewed after some time to see how well it
has held up to fully recognize the value, and to continue from any flaws
identified over time. This can form the baseis for another tech talk. Discussion
of the thinking behind this modeling can form the basis of a public technical
talk. If the content is secret sauce to the organization than it helps to engage
in weekly (or regular, on some schedule) internal tech talks. Like before,
sharing these insights magnifies the value to the organization so a talk here
can turn a small value into a larger one.&lt;/p&gt;
&lt;p&gt;The third scenario would typically unfold as a documented effort involving many
steps, from insight to evaluation of impact and convincing team membes of the
value and possible metrics development to preparing a pitch to leaders outside
of the team and possibly delivering that pitch. Then it extends into execution
and possibly monitoring metrics to evaluate the project's level of success. This
one typically leaves a long trail of breadcrumbs and is easy to recall later but
it's worth pointing out that taking the time to incorporate metrics meaningful
to the business pays big dividends here and this also makes foder for internal
tech talks, and also for presentations or white papers or blog entries with a
broader audience. There is significant value to the organization which are
possible in a story like this one, so following up with these presentations and
writings to get the most out of it can carry and outsized benefit.&lt;/p&gt;
&lt;p&gt;The final scenario is the most nebulous to recognize, is hard to measure (the
metrics can become contaminated by other influences over such a time span), and
naturally the slow osmosis effect can spread....&lt;/p&gt;
&lt;hr class="docutils"&gt;
&lt;p&gt;In the next part I will explore .&lt;/p&gt;
&lt;blockquote&gt;
&lt;table class="docutils footnote" frame="void" id="id2" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="https://stevelle.me/posts/learning-as-performance-2/#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a href="https://stevelle.me/posts/learning-as-performance-2/#id3"&gt;&lt;span class="problematic" id="id4"&gt;`Learning As Performance, part 1
&amp;lt;&amp;gt;`_&lt;/span&gt;&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;/div&gt;
&lt;div class="system-messages section"&gt;
&lt;h2&gt;Docutils System Messages&lt;/h2&gt;
&lt;div class="system-message" id="id3"&gt;
&lt;p class="system-message-title"&gt;System Message: ERROR/3 (&lt;tt class="docutils"&gt;&amp;lt;string&amp;gt;&lt;/tt&gt;, line 82); &lt;em&gt;&lt;a href="https://stevelle.me/posts/learning-as-performance-2/#id4"&gt;backlink&lt;/a&gt;&lt;/em&gt;&lt;/p&gt;
Unknown target name: "learning as performance, part 1 &amp;lt;&amp;gt;".&lt;/div&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>performance_management</category><category>thinkings</category><guid>https://stevelle.me/posts/learning-as-performance-2/</guid><pubDate>Sat, 18 Feb 2017 01:09:03 GMT</pubDate></item><item><title>Learning As Performance, part 1</title><link>https://stevelle.me/posts/learning-as-performance-1/</link><dc:creator>Steve Lewis</dc:creator><description>&lt;div&gt;&lt;p&gt;Let's talk about performance reviews &lt;a class="footnote-reference" href="https://stevelle.me/posts/learning-as-performance-1/#id4" id="id1"&gt;[1]&lt;/a&gt;. Consistently, performance reviews
have been uncomfortable for me. I feel awkward about the standard questions used
in reviews and in self-assessments and don't want to go through the process
because there will always be some uncomfortable moments in there which would be
easier to not revisit. Yet they remain a common feature of almost all
organizations of any size and they keep happening so if performance is going to
be reviewed, I would like to make it meaningful. What should "performance" mean
then, and what does it suggest about my career advancement?&lt;/p&gt;
&lt;div class="section" id="the-problem-with-performance"&gt;
&lt;h2&gt;The Problem with Performance&lt;/h2&gt;
&lt;p&gt;As a coder, my performance could take the form of having done some body of work.
Let's say I built some features and I fixed some bugs. This is the 'what' of the
work. Should you gauge my performance by the particular work done? Then we must
ask more questions: How did the work I completed get selected? Did I choose
politically sensitive work to garner recognition and did I appear to succeed in
the work of did the value of the work under perform expectations? All of this
can be corrupted by independant variables and any measure can be gamed which
means instead of measuring my performance you are effectively measuring the
weather.&lt;/p&gt;
&lt;p&gt;For piece-work jobs this may be "good enough" but for knowledge workers the
organization's primary metrics might not be what the best criteria to gague
individual performance. For knowledge workers to keep your organization growing
and to accelerate that growth to keep ahead of competitors you need their
productivity to grow over time. The difficult fact is that given the
increasing cost of maintaining software over time as feature bloat, technical
debt, and code rot set in often causes a slowing of productivity &lt;a class="footnote-reference" href="https://stevelle.me/posts/learning-as-performance-1/#id5" id="id2"&gt;[2]&lt;/a&gt;. This
makes it even more dangerous for your knowledge workers to be just treading
water. Learning is obligatory.&lt;/p&gt;
&lt;p&gt;Another measure of the value of my work in an organization is in 'how' I did the
work. I'd like to suggest that how I do the work, specifically what I learned
from doing the work, is a better indicator of performance and value to the
organization. This could be a basis for performance review and considering
recognition for achievements (and if your org does titles maybe this can be
valuable in marking advancement in titles too &lt;a class="footnote-reference" href="https://stevelle.me/posts/learning-as-performance-1/#id6" id="id3"&gt;[3]&lt;/a&gt;).&lt;/p&gt;
&lt;/div&gt;
&lt;div class="section" id="learning-as-performance"&gt;
&lt;h2&gt;Learning as Performance&lt;/h2&gt;
&lt;p&gt;One of the rewards of working on software is that there are a lot of
opportunities to solve new problems all the time --variety! A former coworker
once observed (I paraphrase) that our job is to be struggling, to be challenged.
As soon as we overcome one challenge, there is another one waiting for us and
it's time to move on to that.&lt;/p&gt;
&lt;p&gt;We are going to spend the great majority of our time struggling through that
next thing. We can mark our professional development in part by our raw
productivity, but the greater part of our professional growth comes from the
breakthroughs: the monster bugs fixed or the finesse of implementing a feature.&lt;/p&gt;
&lt;p&gt;If my daily routine is going to be one continuous learning process, and that
learning is the function by which I increase my technical capabilities and
productivity, then this is a fair measure of my increasing value as an employee
to the organization. If my organization values individual capability and
productivity increasing, let's think about how we can reflect that in setting
career paths, marking individual and team achievements, and conducting
performance reviews.&lt;/p&gt;
&lt;p&gt;What could be gained by looking at reviews through the lense of learnings?&lt;/p&gt;
&lt;p&gt;By including, or centering on, learned lessons in performance reviews we can
discuss all of our work in non-threatening terms; finding insights and lessons
and discoveries rather than letting the hard times be seen as oversights,
failures, or mistakes.&lt;/p&gt;
&lt;hr class="docutils"&gt;
&lt;p&gt;In the next part I will explore ways to recognize and keep track of the
individual learnings so they are available at performance review time.&lt;/p&gt;
&lt;blockquote&gt;
&lt;table class="docutils footnote" frame="void" id="id4" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="https://stevelle.me/posts/learning-as-performance-1/#id1"&gt;[1]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="https://en.wikipedia.org/wiki/Performance_appraisal"&gt;Wikipedia, Performance Appraisal&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id5" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="https://stevelle.me/posts/learning-as-performance-1/#id2"&gt;[2]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://static1.1.sqspcdn.com/static/f/702523/9243601/1288747638857/200712-Jones.pdf"&gt;"Geriatric Issues of Aging Software" by Casper Jones&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table class="docutils footnote" frame="void" id="id6" rules="none"&gt;
&lt;colgroup&gt;&lt;col class="label"&gt;&lt;col&gt;&lt;/colgroup&gt;
&lt;tbody valign="top"&gt;
&lt;tr&gt;&lt;td class="label"&gt;&lt;a class="fn-backref" href="https://stevelle.me/posts/learning-as-performance-1/#id3"&gt;[3]&lt;/a&gt;&lt;/td&gt;&lt;td&gt;&lt;a class="reference external" href="http://randsinrepose.com/archives/titles-are-toxic/"&gt;Rands in Repose blog: Titles are Toxic:&lt;/a&gt;&lt;/td&gt;&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;/blockquote&gt;
&lt;/div&gt;&lt;/div&gt;</description><category>performance_management</category><category>thinkings</category><guid>https://stevelle.me/posts/learning-as-performance-1/</guid><pubDate>Sat, 11 Feb 2017 03:25:51 GMT</pubDate></item></channel></rss>