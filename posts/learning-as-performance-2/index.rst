.. title: Learning As Performance, part 2
.. slug: learning-as-performance-2
.. date: 2017-02-17 17:09:03 UTC-08:00
.. tags: performance_management, thinkings
.. category: code
.. link:
.. description:
.. type: text

In the first part [1]_, I explored the topic of performance reviews, and I
proposed that learnings could be a meaningful and constructive review criteria.
This time I want to explore ways to recognize and keep track of the individual
learnings so they are available at performance review time.

Record Keeping
--------------

To give a performance review based on "things learned" it is important to have
the list of "things". This gets to one of the hardest parts about
self-assessment for me: keeping a good record of activity. That activity can
take many forms, not limited to:

  - close-up actions like applying a point change for a performance issue
    triggered by learning more about some technical topic
  - inspired decisions like influencing the modeling of a component based on
    learning about the nature of the vendors your customers work with
  - big picture perspectives like from recognizing patterns in bug reports which
    helps to justify important rework to improve product quality and lower
    support costs
  - thematic concerns such as developing excellent insights for code reviews,
    spurred by accumulated experiences about how the product changes over time,
    when leads to coding decisions which make long term maintenance and change
    easier

Some of these are easier to recognize in the moment and possibly to quantify,
but at the same time harder to keep track of or value over several months to a
year later when it comes time for a performance review.

Others take a longer view to recognize occuring, and sometimes even longer views
to fully realize the value from (and quantifying the value can get really sticky
as well). In those cases it can be helpful to have a good list of things you
have been doing and to have someone to review them with as a collaborative
effort to recognize and fully appreciate.


It is important to articulate two things for each thing brought to a review:
  - The magnitude of the value gained, and the learning that facilitated it. An
    estimate is more valuable than abstract language but hard metrics are best.
  - Where the thinking, the learning, and the lessons were shared. Shared
    learning applies a value multiplier.

A scenario like the first stands to benefit the whole team if the technical
learning is shared and the team gains the ability to recognize the cause and to
apply a solution to similar performance issues. A brown-bag seminar could be a
great way to share that learning, and the calendar event serves as a record of
the activity.

In the second scenario reflects a decision made in the normal course of doing
the work. It is possible that commit message or notes in a bug tracker might
capture some of the insight and reasoning behind it. The insight and the
resulting model likely needs to be reviewed after some time to see how well it
has held up to fully recognize the value, and to continue from any flaws
identified over time. This can form the baseis for another tech talk. Discussion
of the thinking behind this modeling can form the basis of a public technical
talk. If the content is secret sauce to the organization than it helps to engage
in weekly (or regular, on some schedule) internal tech talks. Like before,
sharing these insights magnifies the value to the organization so a talk here
can turn a small value into a larger one.

The third scenario would typically unfold as a documented effort involving many
steps, from insight to evaluation of impact and convincing team membes of the
value and possible metrics development to preparing a pitch to leaders outside
of the team and possibly delivering that pitch. Then it extends into execution
and possibly monitoring metrics to evaluate the project's level of success. This
one typically leaves a long trail of breadcrumbs and is easy to recall later but
it's worth pointing out that taking the time to incorporate metrics meaningful
to the business pays big dividends here and this also makes foder for internal
tech talks, and also for presentations or white papers or blog entries with a
broader audience. There is significant value to the organization which are
possible in a story like this one, so following up with these presentations and
writings to get the most out of it can carry and outsized benefit.

The final scenario is the most nebulous to recognize, is hard to measure (the
metrics can become contaminated by other influences over such a time span), and
naturally the slow osmosis effect can spread....

-----

In the next part I will explore .

  .. [1] `Learning As Performance, part 1
    <>`_
